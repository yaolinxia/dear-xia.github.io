---
layout: post
title: "RNN"
tag: 机器学习
---

### 简介

![img](https://pic4.zhimg.com/80/v2-f716c816d46792b867a6815c278f11cb_hd.jpg)

![x](https://www.zhihu.com/equation?tex=x) 为当前状态下数据的输入， ![h](https://www.zhihu.com/equation?tex=h) 表示接收到的上一个节点的输入。

![y](https://www.zhihu.com/equation?tex=y) 为当前节点状态下的输出，而 ![h'](https://www.zhihu.com/equation?tex=h%27) 为传递到下一个节点的输出。

通过上图的公式可以看到，输出 **h'** 与 **x** 和 **h** 的值都相关。

而 **y** 则常常使用 **h'** 投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。

#### 序列形式的表现，如下

![img](https://pic2.zhimg.com/80/v2-71652d6a1eee9def631c18ea5e3c7605_hd.jpg)

### LSTM

- 是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失以及梯度爆炸问题。

- 相比普通的RNN，LSTM能够在更长的序列中有更好的表现

![img](https://pic4.zhimg.com/80/v2-e4f9851cad426dfe4ab1c76209546827_hd.jpg)

相比RNN只有一个传递状态 ![h^t ](https://www.zhihu.com/equation?tex=h%5Et+) ，LSTM有两个传输状态，一个 ![c^t](https://www.zhihu.com/equation?tex=c%5Et) （cell state），和一个 ![h^t](https://www.zhihu.com/equation?tex=h%5Et)（hidden state）。（Tips：RNN中的 ![h^t](https://www.zhihu.com/equation?tex=h%5Et) 对于LSTM中的 ![c^t](https://www.zhihu.com/equation?tex=c%5Et) ）

其中对于传递下去的 ![c^t](https://www.zhihu.com/equation?tex=c%5Et) 改变得很慢，通常输出的 ![c^t](https://www.zhihu.com/equation?tex=c%5Et) 是上一个状态传过来的 ![c^{t-1}](https://www.zhihu.com/equation?tex=c%5E%7Bt-1%7D) 加上一些数值。

而 ![h^t](https://www.zhihu.com/equation?tex=h%5Et) 则在不同节点下往往会有很大的区别。

### 深入LSTM结构

下面具体对LSTM的内部结构来进行剖析。

首先使用LSTM的当前输入 ![x^t](https://www.zhihu.com/equation?tex=x%5Et) 和上一个状态传递下来的 ![h^{t-1}](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) 拼接训练得到四个状态。

![img](https://pic4.zhimg.com/80/v2-15c5eb554f843ec492579c6d87e1497b_hd.jpg)

![img](https://pic1.zhimg.com/80/v2-d044fd0087e1df5d2a1089b441db9970_hd.jpg)

其中， ![z^f ](https://www.zhihu.com/equation?tex=z%5Ef+) ， ![z^i](https://www.zhihu.com/equation?tex=z%5Ei) ，![z^o](https://www.zhihu.com/equation?tex=z%5Eo) 是由拼接向量乘以权重矩阵之后，再通过一个 ![sigmoid ](https://www.zhihu.com/equation?tex=sigmoid+) 激活函数转换成0到1之间的数值，来作为一种门控状态。而 ![z](https://www.zhihu.com/equation?tex=z) 则是将结果通过一个 ![tanh](https://www.zhihu.com/equation?tex=tanh) 激活函数将转换成-1到1之间的值（这里使用 ![tanh](https://www.zhihu.com/equation?tex=tanh) 是因为这里是将其做为输入数据，而不是门控信号）。

![img](https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_hd.jpg)

![\odot](https://www.zhihu.com/equation?tex=%5Codot) 是Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。 ![\oplus](https://www.zhihu.com/equation?tex=%5Coplus) 则代表进行矩阵加法。

#### 内部三个阶段

LSTM内部主要有三个阶段：

\1. 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行**选择性**忘记。简单来说就是会 “忘记不重要的，记住重要的”。

具体来说是通过计算得到的 ![z^f](https://www.zhihu.com/equation?tex=z%5Ef) （f表示forget）来作为忘记门控，来控制上一个状态的 ![c^{t-1}](https://www.zhihu.com/equation?tex=c%5E%7Bt-1%7D) 哪些需要留哪些需要忘。

\2. 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 ![x^t](https://www.zhihu.com/equation?tex=x%5Et) 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 ![z ](https://www.zhihu.com/equation?tex=z+) 表示。而选择的门控信号则是由 ![z^i](https://www.zhihu.com/equation?tex=z%5Ei) （i 代表information）来进行控制。

> 将上面两步得到的结果相加，即可得到传输给下一个状态的 ![c^t](https://www.zhihu.com/equation?tex=c%5Et) 。也就是上图中的第一个公式。

\3. 输出阶段。这个阶段将决定哪些将会被当成**当前状态**的输出。主要是通过 ![z^o](https://www.zhihu.com/equation?tex=z%5Eo) 来进行控制的。并且还对上一阶段得到的 ![c^o](https://www.zhihu.com/equation?tex=c%5Eo) 进行了放缩（通过一个tanh激活函数进行变化）。

与普通RNN类似，输出 ![y^t](https://www.zhihu.com/equation?tex=y%5Et) 往往最终也是通过 ![h^t](https://www.zhihu.com/equation?tex=h%5Et) 变化得到。

### GRU

GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了**解决长期记忆**和**反向传播中的梯度**等问题而提出来的。

#### 优势

![img](https://pic4.zhimg.com/80/v2-a8424cd80eae1b7d312991692decbe8b_hd.jpg)

更容易进行计算

#### 输入结构

GRU的输入输出结构与普通的RNN是一样的。

有一个当前的输入 ![x^t](https://www.zhihu.com/equation?tex=x%5Et) ，和上一个节点传递下来的隐状态（hidden state） ![h^{t-1}](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) ，这个隐状态包含了之前节点的相关信息。

结合 ![x^t ](https://www.zhihu.com/equation?tex=x%5Et+) 和 ![h^{t-1}](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D)，GRU会得到当前隐藏节点的输出 ![y^t ](https://www.zhihu.com/equation?tex=y%5Et+) 和传递给下一个节点的隐状态 ![h^t](https://www.zhihu.com/equation?tex=h%5Et) 。

![img](https://pic2.zhimg.com/80/v2-49244046a83e30ef2383b94644bf0f31_hd.jpg)

#### 内部结构

通过上一个传输下来的状态 ![h^{t-1}](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) 和当前节点的输入 ![x^t](https://www.zhihu.com/equation?tex=x%5Et) 来获取两个门控状态。如下图，其中 ![r ](https://www.zhihu.com/equation?tex=r+) 控制重置的门控（reset gate）， ![z](https://www.zhihu.com/equation?tex=z) 为控制更新的门控（update gate）。

> Tips： ![\sigma](https://www.zhihu.com/equation?tex=%5Csigma) 为*sigmoid*函数，通过这个函数可以将数据变换为0-1范围内的数值，从而来充当门控信号。

![img](https://pic3.zhimg.com/80/v2-7fff5d817530dada1b279c7279d73b8a_hd.jpg)

得到门控信号之后，首先使用重置门控来得到**“重置”**之后的数据 ![{h^{t-1}}' = h^{t-1} \odot r ](https://www.zhihu.com/equation?tex=%7Bh%5E%7Bt-1%7D%7D%27+%3D+h%5E%7Bt-1%7D+%5Codot+r+) ，再将 ![{h^{t-1}}'](https://www.zhihu.com/equation?tex=%7Bh%5E%7Bt-1%7D%7D%27) 与输入 ![x^t ](https://www.zhihu.com/equation?tex=x%5Et+) 进行拼接，再通过一个[tanh](http://link.zhihu.com/?target=https%3A//baike.baidu.com/item/tanh)激活函数来将数据放缩到**-1~1**的范围内。即得到如下图所示的 ![h'](https://www.zhihu.com/equation?tex=h%27) 。

![img](https://pic4.zhimg.com/80/v2-390781506bbebbef799f1a12acd7865b_hd.jpg)

这里的 ![h' ](https://www.zhihu.com/equation?tex=h%27+) 主要是包含了当前输入的 ![x^t](https://www.zhihu.com/equation?tex=x%5Et) 数据。有针对性地对 ![h'](https://www.zhihu.com/equation?tex=h%27) 添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“。类似于LSTM的选择记忆阶段（参照我的上一篇文章）。

![img](https://pic1.zhimg.com/80/v2-8134a00c243153bfd9fd2bcbe0844e9c_hd.jpg)

> 图2-3中的 ![\odot](https://www.zhihu.com/equation?tex=%5Codot) 是Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。 ![\oplus](https://www.zhihu.com/equation?tex=%5Coplus) 则代表进行矩阵加法操作。

最后介绍GRU最关键的一个步骤，我们可以称之为**”更新记忆“**阶段。

在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控 ![z](https://www.zhihu.com/equation?tex=z) （update gate）。

**更新表达式**： ![h^t = z \odot h^{t-1} + (1 - z)\odot h'](https://www.zhihu.com/equation?tex=h%5Et+%3D+z+%5Codot+h%5E%7Bt-1%7D+%2B+%281+-+z%29%5Codot+h%27)

首先再次强调一下，门控信号（这里的 ![z](https://www.zhihu.com/equation?tex=z) ）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。



GRU很聪明的一点就在于，**我们使用了同一个门控 ![z](https://www.zhihu.com/equation?tex=z) 就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）**。

- ![z \odot h^{t-1}](https://www.zhihu.com/equation?tex=z+%5Codot+h%5E%7Bt-1%7D) ：表示对原本隐藏状态的选择性“遗忘”。这里的 ![z](https://www.zhihu.com/equation?tex=z) 可以想象成遗忘门（forget gate），忘记 ![h^{t-1}](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D) 维度中一些不重要的信息。
- ![(1-z) \odot h'](https://www.zhihu.com/equation?tex=%281-z%29+%5Codot+h%27) ： 表示对包含当前节点信息的 ![h'](https://www.zhihu.com/equation?tex=h%27) 进行选择性”记忆“。与上面类似，这里的 ![(1-z)](https://www.zhihu.com/equation?tex=%281-z%29) 同理会忘记 ![h '](https://www.zhihu.com/equation?tex=h+%27) 维度中的一些不重要的信息。或者，这里我们更应当看做是对 ![h' ](https://www.zhihu.com/equation?tex=h%27+) 维度中的某些信息进行选择。
- ![h^t = z \odot h^{t-1} + (1 - z)\odot h'](https://www.zhihu.com/equation?tex=h%5Et+%3D+z+%5Codot+h%5E%7Bt-1%7D+%2B+%281+-+z%29%5Codot+h%27) ：结合上述，这一步的操作就是忘记传递下来的 ![h^{t-1} ](https://www.zhihu.com/equation?tex=h%5E%7Bt-1%7D+) 中的某些维度信息，并加入当前节点输入的某些维度信息。

> 可以看到，这里的遗忘 ![z](https://www.zhihu.com/equation?tex=z) 和选择 ![(1-z)](https://www.zhihu.com/equation?tex=%281-z%29) 是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （![z](https://www.zhihu.com/equation?tex=z) ），我们就会使用包含当前输入的 ![h'](https://www.zhihu.com/equation?tex=h%27) 中所对应的权重进行弥补 ![(1-z)](https://www.zhihu.com/equation?tex=%281-z%29) 。以保持一种”恒定“状态。





### 参考文献

- <https://zhuanlan.zhihu.com/p/32085405>(LSTM)
- <https://zhuanlan.zhihu.com/p/32481747>(GRU)