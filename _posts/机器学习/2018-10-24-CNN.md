---
layout: post
title: "CNN"
tag: 机器学习
---

## 特点：

- 卷积层与全连接层的区别，是可以保全空间结构
- 此处卷积其实和点积没有区别
- 黑白图片：还只有一个颜色通道；彩色图片：有三个颜色通道
- 传统神经网络：全连接层；CNN：局部连接，权值共享

![](https://ws1.sinaimg.cn/large/e93305edgy1fwj30oimk3j212t0lknbe.jpg)



## 卷积如何遍历：

- 会使用一组卷积核

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj3ho64q1j213m0mg17d.jpg)

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj3j4j7r3j20x80k64a1.jpg)

  ~~~
  32-5+1=28
  ~~~

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj3kp42zgj211d0kitjd.jpg)

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj3xoctaxj21450my4qp.jpg)

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj3zpz7w0j212j0le1kx.jpg)

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj42kxzmaj20u40m0wmc.jpg)

  ~~~
  步长为1
  ~~~

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj43l4lgij20z70irjwr.jpg)

  ~~~
  步长为2
  ~~~

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj447g5rhj20yw0i6k1w.jpg)

  ~~~
  步长为3，不能很好地拟合
  ~~~

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj456e4goj21240in0zj.jpg)

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj4bjka4dj21160latk1.jpg)

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj4koyd3vj212y0jrqe3.jpg)

  ![](https://ws1.sinaimg.cn/large/e93305edgy1fwj56jzld0j20z00iy7jq.jpg)

## 局部连接与权值共享：

下图是一个很经典的图示，左边是全连接，右边是局部连接。

  [![img](http://i.imgur.com/PHbta3D.jpg)](http://i.imgur.com/PHbta3D.jpg)

- 对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。

- **进一步减少数量级：权值共享**。在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，**将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同**，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 **10 × 10个权值参数**（也就是卷积核(也称滤波器)的大小），如下图。

  [![img](http://i.imgur.com/IIBM59H.jpg)](http://i.imgur.com/IIBM59H.jpg)

- 这大概就是CNN的一个神奇之处，尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，可以增加**多个卷积核**，不同的卷积核能够得到图像的不同映射下的特征，称之为**Feature Map**。如果有100个卷积核，最终的权值参数也仅为100 × 100 = 10^4个而已。另外，偏置参数也是共享的，同一种滤波器（卷积核）共享一个。

- **核心思想：**局部感受野

## 网络结构：

### LetNet-5结构：

![img](http://i.imgur.com/qMs50Ma.png)

- CNN中主要有两种类型的网络层，分别是**卷积层**和**池化/采样层(Pooling)**。卷积层的作用是提取图像的各种特征；池化层的作用是对原始特征信号进行抽象，从而大幅度减少训练参数，另外还可以减轻模型过拟合的程度。

## 卷积层：

- 卷积层是**卷积核**在上一级输入层上通过逐一滑动窗口计算而得，卷积核中的每一个参数都相当于传统神经网络中的权值参数，与对应的局部像素相连接，将卷积核的各个参数与对应的局部像素值相乘之和，（通常还要再加上一个偏置参数），得到卷积层上的结果。如下图所示。

[![img](http://i.imgur.com/w8enPv2.png)](http://i.imgur.com/w8enPv2.png)

下面的动图能够更好地解释卷积过程：

[![img](http://i.imgur.com/KPyqPOB.gif)](http://i.imgur.com/KPyqPOB.gif)

## 池化/采样层：

- 通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如**softmax**），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行**池化/采样(Pooling)**处理。池化/采样的方式通常有以下两种：

- **Max-Pooling**: 选择Pooling窗口中的最大值作为采样值；
- **Mean-Pooling**: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值；

如下图所示。

[![img](http://i.imgur.com/bHBUsr4.png)](http://i.imgur.com/bHBUsr4.png)

### LeNet-5网络详解

以上较详细地介绍了CNN的网络结构和基本原理，下面介绍一个经典的CNN模型：**LeNet-5网络**。

[![img](http://i.imgur.com/2AuotA0.png)](http://i.imgur.com/2AuotA0.png)

[![img](http://i.imgur.com/Zzm048o.png)](http://i.imgur.com/Zzm048o.png)

[![img](http://i.imgur.com/aJlgVHg.png)](http://i.imgur.com/aJlgVHg.png)

](http://i.imgur.com/SiVPyWR.png)

[![img](http://i.imgur.com/gTphBu6.png)](http://i.imgur.com/gTphBu6.png)

[![img](http://i.imgur.com/6L3CmUc.png)](http://i.imgur.com/6L3CmUc.png)

[![img](http://i.imgur.com/SNLgNWe.png)](http://i.imgur.com/SNLgNWe.png)

[![img](http://i.imgur.com/kJtbaEz.png)](http://i.imgur.com/kJtbaEz.png)

**LeNet-5网络在MNIST数据集上的结果**

[![img](http://i.imgur.com/cXSxkVY.png)](http://i.imgur.com/cXSxkVY.png)



### 参考网址：

- <http://www.jeyzhang.com/cnn-learning-notes-1.html>
- 