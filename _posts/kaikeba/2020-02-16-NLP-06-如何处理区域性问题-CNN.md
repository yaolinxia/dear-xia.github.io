**Froward and backward propagations**
SGD
Training scheme and some training tips
Some terminologies
    Epoch 
    Iteration 
    Hyperparameters
    Parameters

![image-20200216202200889](../yaolinxia.github.io/img/image-20200216202200889.png)

L1: 会使得选择更加稀疏

L2:不会那么稀疏

![image-20200216202344756](../yaolinxia.github.io/img/image-20200216202344756.png)



Convolutional Neural Networks 
What is CNN.
Pooling
Training tips 
Some Famous CNN Models
LetNet
AlexNet
GoogleNet
VGG, ResNet
DenseNet
Coding
Introduction of keras

![image-20200216204917956](../yaolinxia.github.io/img/image-20200216204917956.png)

参数共享好处：

平移不变形





## padding 

- 使用好处： 可以保持纬度

![image-20200216211141333](../yaolinxia.github.io/img/image-20200216211141333.png)

![image-20200216211548778](../yaolinxia.github.io/img/image-20200216211548778.png)

使用samepadding好处，可以保持纬度不变

![image-20200216212505253](../yaolinxia.github.io/img/image-20200216212505253.png)

进行一个向下取整
卷积核是一个超参，取决于后面的训练



## 感受野

用来计算输出值的像素值范围

![image-20200216212850089](../yaolinxia.github.io/img/image-20200216212850089.png)

越往上叠，感受视野越大



## pooling

**好处：**降纬、不引入任何参数

![image-20200216213353945](../yaolinxia.github.io/img/image-20200216213353945.png)

## feature-map

- **如果是在一个多维的图片上进行卷积**
- 输入的channel要和卷积核的channel一致，用到多少个卷积核，就有多少个channel
- ![image-20200216214924514](../yaolinxia.github.io/img/image-20200216214924514.png)

![image-20200216221609713](../yaolinxia.github.io/img/image-20200216221609713.png)



高层的feature是基于底层的feature进行卷积

## 其他CNN

- 高纬到地纬变换

![image-20200216221907611](../yaolinxia.github.io/img/image-20200216221907611.png)

![image-20200216223814685](../yaolinxia.github.io/img/image-20200216223814685.png)

![image-20200216223618446](../yaolinxia.github.io/img/image-20200216223618446.png)

好处： 

- 简单的可以直接传
- 复杂的可以进一步，训练深层网络
- 

## textCNN

![image-20200216224205929](../yaolinxia.github.io/img/image-20200216224205929.png)

最终可以得到整个句子的表示